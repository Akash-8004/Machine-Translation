{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae53976-7acb-4c19-9eb4-80c826dd833e",
   "metadata": {},
   "source": [
    "# Machine Translation Project (English- French)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74feb9-9ccb-43b4-98b8-d6bceac11ca8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35abc3d3-981e-4a25-b66c-f9bbdbd52840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras. layers import Input, Dense, Embedding, GRU, LSTM, Bidirectional, Dropout, Activation, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f88370b1-16a0-4929-8976-3e10d2dee596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: 1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002db7e7-28f9-4c01-b1c5-58ed1e8f8ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15800800925622539688\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4408931328\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15410409250742514012\n",
      "physical_device_desc: \"device: 0, name: DML, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4720c3-3933-48a3-a765-d669bd094f9f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c348268-fcae-400a-8d4f-84e5bbfc793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "english_sen = load_data(\"data/english.txt\")\n",
    "french_sen = load_data(\"data/french.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa19dad2-159d-4693-9d8e-5033e625e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .', 'the united states is usually chilly during july , and it is usually freezing in november .', 'california is usually quiet during march , and it is usually hot in june .', 'the united states is sometimes mild during june , and it is cold in september .', 'your least liked fruit is the grape , but my least liked is the apple .']\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\", 'les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .', 'california est gÃ©nÃ©ralement calme en mars , et il est gÃ©nÃ©ralement chaud en juin .', 'les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et il fait froid en septembre .', 'votre moins aimÃ© fruit est le raisin , mais mon moins aimÃ© est la pomme .']\n"
     ]
    }
   ],
   "source": [
    "print(english_sen[0:5])\n",
    "print(french_sen[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fc4c1-a4ce-4f54-a50c-b172fa10eec0",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665d8101-2e09-4ce8-a561-cdb7ec221091",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sen_words = []\n",
    "french_sen_words = []\n",
    "for sentences in english_sen:\n",
    "    for words in sentences.split():\n",
    "        english_sen_words.append(words)\n",
    "english_sen_words_counter = collections.Counter(english_sen_words)\n",
    "\n",
    "for sentences in french_sen:\n",
    "    for words in sentences.split():\n",
    "        french_sen_words.append(words)\n",
    "french_sen_words_counter = collections.Counter(french_sen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d2caac-de42-4a6d-adda-7a7e403981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: of words in english dataset: 1823250\n",
      "No: of Unique words in english dataset: 227\n",
      "Top 10 most Common english words: ['is', ',', '.', 'in', 'it', 'during', 'the', 'but', 'and', 'sometimes']\n",
      "\n",
      "No: of words in french dataset: 1961295\n",
      "No: of Unique words in french dataset: 355\n",
      "Top 10 most Common french words: ['est', '.', ',', 'en', 'il', 'les', 'mais', 'et', 'la', 'parfois']\n"
     ]
    }
   ],
   "source": [
    "print(f\"No: of words in english dataset: {len(english_sen_words)}\")\n",
    "print(f\"No: of Unique words in english dataset: {len(english_sen_words_counter)}\")\n",
    "print(f\"Top 10 most Common english words: {[key for key, value in english_sen_words_counter.most_common(10)]}\")\n",
    "print()\n",
    "print(f\"No: of words in french dataset: {len(french_sen_words)}\")\n",
    "print(f\"No: of Unique words in french dataset: {len(french_sen_words_counter)}\")\n",
    "print(f\"Top 10 most Common french words: {[key for key, value in french_sen_words_counter.most_common(10)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2ff9-343f-4081-9533-c0419908c0c0",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a476bb-fe4f-431d-9c84-61fd11e95018",
   "metadata": {},
   "source": [
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40ee3cf-f824-4ae0-9d9d-93579f9d7b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x000002229B7EF310>\n",
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "[[1, 2, 4, 5, 6, 7, 1, 8, 9], [10, 11, 12, 2, 13, 14, 15, 16, 3, 17], [18, 19, 3, 20, 21]]\n",
      "Sequence 1 in text_sentences\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in text_sentences\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in text_sentences\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer)\n",
    "print(text_tokenizer.word_index)\n",
    "print(text_tokenized)\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print(f'Sequence {sample_i + 1} in text_sentences')\n",
    "    print(f'  Input:  {sent}')\n",
    "    print(f'  Output: {token_sent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2bfb5a2-8aed-420e-9447-112090409d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "  Original: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "  Padded:   [1 2 4 5 6 7 1 8 9 0]\n",
      "Sentence 2\n",
      "  Original: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "  Padded:   [10 11 12  2 13 14 15 16  3 17]\n",
      "Sentence 3\n",
      "  Original: [18, 19, 3, 20, 21]\n",
      "  Padded:   [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pad_simple(sentences, length=None):\n",
    "    # if user did NOT specify length, use max sentence length\n",
    "    if length is None:\n",
    "        length = max(len(s) for s in sentences)\n",
    "\n",
    "    # pad all sentences to THIS length\n",
    "    return pad_sequences(sentences, maxlen=length, padding='post')\n",
    "\n",
    "# Apply padding\n",
    "padded = pad_simple(text_tokenized)\n",
    "\n",
    "# Print original vs padded\n",
    "for i, (orig, padded_sent) in enumerate(zip(text_tokenized, padded), 1):\n",
    "    print(f\"Sentence {i}\")\n",
    "    print(\"  Original:\", orig)\n",
    "    print(\"  Padded:  \", padded_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db40d34-532a-40aa-90d8-29355c8d0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x,y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    preprocess_x = pad_simple(preprocess_x)\n",
    "    preprocess_y = pad_simple(preprocess_y)\n",
    "    \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sen, french_sen)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8ba5df-0fa5-4be8-9639-b70fa5d9146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9845fe-b287-4888-8e03-06b2b3eb2c73",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Model1 is simple RNN\n",
    "- Model2 is Bidirectional RNN\n",
    "- Model3 is an embedding RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb613fc-d4d6-4546-a35d-70a432a9de56",
   "metadata": {},
   "source": [
    "* Since model gives tokans values of french language so it is necessary to convert back them to texts using tokanizer.word_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c1b781-03fb-4929-b5f5-fcc7d208fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    Index_to_word = {id:word for word, id in tokenizer.word_index.items()}\n",
    "    Index_to_word[0] = '<PPAD>'\n",
    "\n",
    "    return ' '.join([Index_to_word[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd92b174-0d13-4ac3-b7c9-1d750c925cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      " 54/108 [==============>...............] - ETA: 1:01 - loss: 2.0359 - accuracy: 0.5676"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m\n\u001b[0;32m     40\u001b[0m y \u001b[38;5;241m=\u001b[39m preproc_french_sentences\n\u001b[0;32m     43\u001b[0m simple_rnn_model \u001b[38;5;241m=\u001b[39m simple_model(\n\u001b[0;32m     44\u001b[0m     max_french_sequence_length,\n\u001b[0;32m     45\u001b[0m     english_vocab_size,\n\u001b[0;32m     46\u001b[0m     french_vocab_size\n\u001b[0;32m     47\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m \u001b[43msimple_rnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[0;32m     55\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\tf-directml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, TimeDistributed, Dense, Dropout, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def simple_model(output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=english_vocab_size,\n",
    "        output_dim=256,\n",
    "        input_length=output_sequence_length\n",
    "    ))\n",
    "\n",
    "    model.add(GRU(\n",
    "        256,\n",
    "        return_sequences=True,\n",
    "        reset_after=False\n",
    "    ))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        optimizer=Adam(learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tmp_x = pad_simple(preproc_english_sentences, length=max_french_sequence_length)\n",
    "tmp_x = tmp_x.astype('float32')   # ❗ VERY IMPORTANT\n",
    "\n",
    "y = preproc_french_sentences\n",
    "\n",
    "\n",
    "simple_rnn_model = simple_model(\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size\n",
    ")\n",
    "\n",
    "simple_rnn_model.fit(\n",
    "    tmp_x,\n",
    "    y,\n",
    "    batch_size=1024,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26916a8b-292e-4f48-aed6-b689748798c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sen[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sen[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af034170-6b4f-49a8-91cf-5db9be68cd2f",
   "metadata": {},
   "source": [
    "## Model 2: Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0033910-0c9b-4c78-8e7e-3edecb6d5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    #Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True, reset_after=False), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad_simple(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(bd_rnn_model.summary())\n",
    "\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd047850-862c-4fb9-835e-b28714fa8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sen[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sen[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d4af8-667b-4e8e-abae-c530110d136d",
   "metadata": {},
   "source": [
    "## Model 3: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02351b54-fb8a-41b9-b617-fd6544ff4180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 256)           50944     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 21, 512)          787968    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 21, 1024)         525312    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 21, 345)          353625    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,849\n",
      "Trainable params: 1,717,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "108/108 [==============================] - 219s 2s/step - loss: 1.3333 - accuracy: 0.6977 - val_loss: 0.4006 - val_accuracy: 0.8743\n",
      "Epoch 2/5\n",
      "108/108 [==============================] - 212s 2s/step - loss: 0.3110 - accuracy: 0.9018 - val_loss: 0.1892 - val_accuracy: 0.9407\n",
      "Epoch 3/5\n",
      "108/108 [==============================] - 202s 2s/step - loss: 0.1807 - accuracy: 0.9438 - val_loss: 0.1328 - val_accuracy: 0.9592\n",
      "Epoch 4/5\n",
      "108/108 [==============================] - 198s 2s/step - loss: 0.1317 - accuracy: 0.9594 - val_loss: 0.1043 - val_accuracy: 0.9684\n",
      "Epoch 5/5\n",
      "108/108 [==============================] - 198s 2s/step - loss: 0.1044 - accuracy: 0.9681 - val_loss: 0.0909 - val_accuracy: 0.9734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222ffad3ee0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bidirectional_embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True, reset_after=False)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad_simple(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Build the model\n",
    "embed_rnn_model = bidirectional_embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(embed_rnn_model.summary())\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6da480-5d14-4afe-b3b1-b68f508c8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PPAD> <PPAD> <PPAD> <PPAD> <PPAD> <PPAD> <PPAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sen[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sen[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e136312b-926c-41c3-9ce7-dcc3ef96a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_rnn_model.save('english_to_french_model.keras')\n",
    "# Serialize English Tokenizer to JSON\n",
    "with open('english_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(english_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Serialize French Tokenizer to JSON\n",
    "with open('french_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(french_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Save max lengths\n",
    "max_french_sequence_length_json = max_french_sequence_length\n",
    "with open('sequence_length.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(max_french_sequence_length_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65f9f9-888a-4c99-9805-5da97486c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdd530-5dcf-4be3-ac77-25a241e038ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow (DirectML)",
   "language": "python",
   "name": "tf-directml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
